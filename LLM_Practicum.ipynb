{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrcN1HP62Dc3"
      },
      "source": [
        "# Notebook Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFpLonhS2G-r",
        "outputId": "6a8e5db3-433b-4300-b68c-fb5fd60c16f3"
      },
      "outputs": [],
      "source": [
        "!pip install onnxruntime >> NULL\n",
        "!pip install -U torch >> NULL\n",
        "!pip install -U sentence_transformers >> NULL\n",
        "!pip install -q -U einops tiktoken accelerate peft bitsandbytes transformers >> NULL\n",
        "\n",
        "print(\"Completed setup\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrL-jQaC2mQg"
      },
      "source": [
        "# LLM Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLb1JCVA278R"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "# Speciy model alias for HF\n",
        "alias = \"NousResearch/Llama-2-7b-chat-hf\"\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(alias, trust_remote_code=True)\n",
        "\n",
        "# Quantization Config\n",
        "quant_config = BitsAndBytesConfig(\n",
        "   load_in_4bit=True,\n",
        "   bnb_4bit_quant_type=\"nf4\",\n",
        "   bnb_4bit_use_double_quant=False,\n",
        "   bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# # Load Model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    alias,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=\"auto\",\n",
        "    quantization_config=quant_config\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0FpLi3o3_Kt"
      },
      "outputs": [],
      "source": [
        "from string import Template\n",
        "\n",
        "prompt_template = Template(\n",
        "    \"\"\"\n",
        "    <s>[INST] <<SYS>>\n",
        "    You are a helpful chatbot.\n",
        "    <</SYS>>\n",
        "    Answer the provided question. Be concise and clear in your response.\n",
        "    $input\n",
        "    [/INST]\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "input = \"Mary has 10 apples. She give 3 to John and 1 to Bob. She threw away 1 more apple. How many apples does Mary have left?\"\n",
        "\n",
        "prompt = prompt_template.substitute({\"input\": input})\n",
        "\n",
        "encoded_prompt = tokenizer(prompt, return_tensors=\"pt\")\n",
        "# encoded_prompt = {k: v.to(\"cuda\") for k,v in encoded_prompt.items()}\n",
        "\n",
        "output = model.generate(**encoded_prompt, max_new_tokens=150)\n",
        "# print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4aS5j55ZOlB"
      },
      "source": [
        "## COT style prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIeXXh5yZRhE"
      },
      "outputs": [],
      "source": [
        "cot_template = Template(\n",
        "  \"\"\"\n",
        "  <s>[INST] <<SYS>>\n",
        "  You are a helpful chatbot.\n",
        "  $input\n",
        "  <</SYS>>\n",
        "  Answer the provided question. Let's think step by step. Plesse provide an step-by-step explanation and then answer the question.\n",
        "  Be concise and clear in your response.\n",
        "  [/INST]\n",
        "  \"\"\"\n",
        ")\n",
        "\n",
        "input = \"Mary has 10 apples. She give 3 to John and 1 to Bob. She threw away 1 more apple. How many apples does Mary have left?\"\n",
        "\n",
        "prompt = cot_template.substitute({\"input\": input})\n",
        "encoded_prompt = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "output = model.generate(**encoded_prompt, max_new_tokens=150)\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXrl6K3jPZoX"
      },
      "source": [
        "# RAG Pipeline\n",
        "\n",
        "## Semantic Similarity Basics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dl4kFnuePg7d",
        "outputId": "584698c6-3f9b-40e1-d861-9e58a2b0b707"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "s1 = \"Cats are super cool.\"\n",
        "s2 = \"Cats are awesome.\"\n",
        "s3 = \"I like felines.\"\n",
        "s4 = \"Centipedes are terrifying.\"\n",
        "\n",
        "s1_embed = embedder.encode(s1).reshape(1,-1)\n",
        "s2_embed = embedder.encode(s2).reshape(1, -1)\n",
        "s3_embed = embedder.encode(s3).reshape(1, -1)\n",
        "s4_embed = embedder.encode(s4).reshape(1, -1)\n",
        "\n",
        "\n",
        "print(f\"Cosine similarity between s1 and s: {cosine_similarity(s1_embed, s2_embed)}\")\n",
        "print(f\"Cosine similarity between s1 and s3: {cosine_similarity(s1_embed, s3_embed)}\")\n",
        "print(f\"Cosine similarity between s1 and s4: {cosine_similarity(s1_embed, s4_embed)}\")\n",
        "\n",
        "# Semantic similarity for retrieval\n",
        "query = \"What is super cool?\"\n",
        "query_embed = embedder.encode(query).reshape(1, -1)\n",
        "\n",
        "print(f\"s1 relevance for query: {cosine_similarity(query_embed, s1_embed)}\")\n",
        "print(f\"s2 relevance for query: {cosine_similarity(query_embed, s2_embed)}\")\n",
        "print(f\"s3 relevance for query: {cosine_similarity(query_embed, s3_embed)}\")\n",
        "print(f\"s4 relevance for query: {cosine_similarity(query_embed, s4_embed)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uf2yU-N4Q_5I"
      },
      "source": [
        "## Simple Dense Passage Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "documents = [\n",
        "    \"The giraffe has 5 spots and 100 stripes.\",\n",
        "    \"The giraffe has blue eyes.\",\n",
        "    \"Giraffes have 4 legs.\",\n",
        "    \"The cat has 6 spots and 200 stripes.\",\n",
        "    \"The cat as green eyes.\",\n",
        "    \"Cats have 4 legs and a tail.\",\n",
        "    \"Penguins have no spots and no stripes.\",\n",
        "    \"Penguins have 2 legs.\",\n",
        "    \"The penguin has emerald eyes\"\n",
        "]\n",
        "\n",
        "\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "query = \"Which animal has blue eyes?\"\n",
        "# 1. Compute similarity between query and doc embeddings\n",
        "query_embed = embedder.encode(query)\n",
        "\n",
        "# 2. Sort documents based on cosine similarity score, order list from\n",
        "# most similar to least\n",
        "idx = embedder.encode(documents)\n",
        "\n",
        "# 3. Retrieve top n documents based on sorted document id\n",
        "sim_scores = cosine_similarity(query_embed.reshape(1, -1), idx)\n",
        "sorted_doc_ids = np.argsort(sim_scores)[0][::-1]\n",
        "\n",
        "# 4. Get documents\n",
        "results = [documents[i] for i in sorted_doc_ids][0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOE93CJjQ_RP"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "from typing import List\n",
        "\n",
        "class SimpleVectorDB:\n",
        "\n",
        "  def __init__(self, documents: List, embedder_alias: str):\n",
        "    self.documents = documents\n",
        "\n",
        "    # Build Index\n",
        "    # a. define the embedder\n",
        "    self.embedder = SentenceTransformer(embedder_alias)\n",
        "\n",
        "    # b. Embed the documents\n",
        "    self.kb = self.embedder.encode(documents)\n",
        "\n",
        "\n",
        "  def fetch_knowledge(self, query: str, n_results: int = 1) -> List[str]:\n",
        "    \"\"\"\n",
        "    Given a user query, retrieve the most relevant document from KB. Retrieval\n",
        "    should be based on the document which is the most semantic relevant to the\n",
        "    query as measured by cosine similarity. Return the top n_results.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Embed the query\n",
        "    query_embed = self.embedder.encode(query).reshape(1,-1)\n",
        "\n",
        "    # 2. Compute similarity between query and docs embeddings\n",
        "    sim_scores = cosine_similarity(query_embed, self.kb)\n",
        "\n",
        "    # 3. Sort documents based on cosine similarity score, order list from\n",
        "    # most similar to least\n",
        "    sorted_doc_ids = np.argsort(sim_scores)[0][::-1]\n",
        "\n",
        "    # 4. Retrieve top n documents based on sorted document id\n",
        "    final_docs = [ self.documents[i] for i in sorted_doc_ids ][:n_results]\n",
        "\n",
        "    return final_docs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdRBaJjZryhH"
      },
      "outputs": [],
      "source": [
        "# Evaluate Retrieval\n",
        "documents = [\n",
        "    \"The giraffe has 5 spots and 100 stripes.\",\n",
        "    \"The giraffe has blue eyes.\",\n",
        "    \"Giraffes have 4 legs.\",\n",
        "    \"The cat has 6 spots and 200 stripes.\",\n",
        "    \"The cat as green eyes.\",\n",
        "    \"Cats have 4 legs and a tail.\",\n",
        "    \"Penguins have no spots and no stripes.\",\n",
        "    \"Penguins have 2 legs.\",\n",
        "    \"The penguin has emerald eyes\"\n",
        "]\n",
        "\n",
        "# Intialize kb\n",
        "kb = SimpleVectorDB(documents, \"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Sample queries\n",
        "q1 = \"Which animal has blue eyes?\"\n",
        "expected_document = \"The giraffe has blue eyes.\"\n",
        "\n",
        "print(q1)\n",
        "print(\"Top documents: \", kb.fetch_knowledge(q1))\n",
        "print(f\"Check: {kb.fetch_knowledge(q1)[0] == expected_document}\")\n",
        "\n",
        "\n",
        "q2 = \"Which animals have atleast 2 legs?\"\n",
        "expected_documents = ['Penguins have 2 legs.', 'Giraffes have 4 legs.', 'Cats have 4 legs and a tail.']\n",
        "print(q2)\n",
        "print(\"Top documents: \", kb.fetch_knowledge(q2, 3))\n",
        "print(f\"Check: { len(set(kb.fetch_knowledge(q2, 3)).intersection(expected_documents)) == 3 }\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5FDW7ET1Ip0"
      },
      "source": [
        "# Retrieval + Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HrmeSgR24gi",
        "outputId": "a2910eb6-38bd-4bf8-8504-b71fec10f8b0"
      },
      "outputs": [],
      "source": [
        "# from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "# tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
        "# model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\", device_map=\"auto\")\n",
        "# prompt_template = Template(\n",
        "# \"\"\"\n",
        "# Answer the provided question below using the provided context.\n",
        "# Context: $context\n",
        "# Question: $question\n",
        "# \"\"\"\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWAGG6D61IBs"
      },
      "outputs": [],
      "source": [
        "from string import Template\n",
        "\n",
        "prompt_template = Template(\n",
        "\"\"\"\n",
        "<s>[INST] <<SYS>>\n",
        "You are a helpful chatbot. \n",
        "Only answer question about animals. If the questions is not about animals, politely respond I can't answer that.\n",
        "<</SYS>>\n",
        "Answer the provided question using the provided context only. Be concise and clear in your response.\n",
        "[/INST]\n",
        "\n",
        "Context:\n",
        "$context\n",
        "\n",
        "Question:\n",
        "$question\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "# 1. Retrieve the top documents\n",
        "q1 = \"How many tires does a ford mustang have?\"\n",
        "docs =  kb.fetch_knowledge(q1,5)\n",
        "\n",
        "# 2. Construct prompt with in-context information\n",
        "#docs = \"\\n\".join(docs)\n",
        "\n",
        "docs = \"\"\n",
        "prompt = prompt_template.substitute({\"context\": docs, \"question\": q1})\n",
        "print('----------')\n",
        "\n",
        "# 3. Prompt model\n",
        "encoded_input = tokenizer(prompt, return_tensors=\"pt\")\n",
        "outputs = model.generate(**encoded_input, max_new_tokens=20)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# QLORA Finetuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "# Instructions\n",
            "Answer the multiple-choice question below. Provide the answer text as the output. \n",
            "\n",
            "Example:\n",
            "Ryan dropped a hammer on his foot. What was the effect?\n",
            "a) he broke his toe \n",
            "b) Ryan scrached his head. \n",
            "Answer: \n",
            "he broke his toe\n",
            "\n",
            "Input\n",
            "the couple was happy to see each other.\n",
            "what was the effect?\n",
            "(a) they rested\n",
            "(b) they kissed\n",
            "Answer:\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd \n",
        "from string import Template \n",
        "from datasets import Dataset\n",
        "from string import Template \n",
        "\n",
        "data = pd.read_csv(\"data/copa-cleaned.csv\")\n",
        "\n",
        "instruction_template = Template(\n",
        "\"\"\"\n",
        "# Instructions\n",
        "Answer the multiple-choice question below. Provide the answer text as the output. \n",
        "\n",
        "Example:\n",
        "Ryan dropped a hammer on his foot. What was the effect?\n",
        "a) he broke his toe \n",
        "b) Ryan scrached his head. \n",
        "Answer: \n",
        "he broke his toe\n",
        "\n",
        "Input\n",
        "$question\n",
        "Answer:\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "\n",
        "# Update the dataset input to match the instruction prompt template \n",
        "data[\"prompt_input\"] = data[\"input\"].apply(\n",
        "    lambda x: instruction_template.substitute({\"question\" : x})\n",
        ")\n",
        "\n",
        "print(data.iloc[0][\"prompt_input\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9408e36630c4430dbe17c77c4426024d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/1500 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "76c90a908da743c08b8dcb1297de0d63",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/1500 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3c323d9d175e463ea6e68fea1a00c5dc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/1500 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# convert pandas dataframe to a dataset object\n",
        "data = Dataset.from_pandas(data)\n",
        "\n",
        "# Extract datasets by split\n",
        "train = data.filter(lambda x: x[\"split\"] == \"train\")\n",
        "val = data.filter(lambda x: x[\"split\"] == \"val\")\n",
        "test = data.filter(lambda x: x[\"split\"] == \"test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Prepare Quantized Model for QLoRA Training.\n",
        "1. Load quantized model\n",
        "2. Convert to LoRA for PEFT training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9a77ec57cba34c328289d2eed9729822",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, BitsAndBytesConfig, set_seed\n",
        "\n",
        "\n",
        "# Speciy model alias for HF\n",
        "alias = \"google/flan-t5-xl\"\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(alias, trust_remote_code=True)\n",
        "\n",
        "#Quantization Config\n",
        "quant_config = BitsAndBytesConfig(\n",
        "   load_in_4bit=True,\n",
        "   bnb_4bit_quant_type=\"nf4\",\n",
        "   bnb_4bit_use_double_quant=False,\n",
        "   \n",
        "   # note for non-ampehere gpus (e.g. T4, V100, RTX) use float16 or leave empty\n",
        "   bnb_4bit_compute_dtype=torch.bfloat16   \n",
        ")\n",
        "\n",
        "# Load Model\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "    alias,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=\"auto\",\n",
        "    quantization_config=quant_config\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM, \n",
        "    inference_mode=False, \n",
        "    target_modules=[\"q\", \"k\", \"v\"],\n",
        "    r=8, \n",
        "    lora_alpha=32, \n",
        "    lora_dropout=0.5\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, peft_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 7,077,888 || all params: 2,856,835,072 || trainable%: 0.2478\n"
          ]
        }
      ],
      "source": [
        "# Note that the trainable parameters are signifacntly smaller. We only training 24% of the model!\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Inference and Training\n",
        "We prepare the models for inference and training in batch setting. First we configure a dataloader that can handle batched inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from transformers import AutoTokenizer, BatchEncoding\n",
        "from ast import literal_eval\n",
        "\n",
        "\n",
        "# The collator is responsible for ensuring the generated batches have a fixed dimension as the \n",
        "#input will be tensor. \n",
        "\n",
        "@dataclass\n",
        "class SimpleCollator:\n",
        "    tokenizer: AutoTokenizer\n",
        "    config: dict \n",
        "    \n",
        "    def __call__(self, examples: list) -> dict:\n",
        "        batch = BatchEncoding(\n",
        "            {\n",
        "                k: [examples[i][k] for i in range(len(examples))]\n",
        "                for k, v in examples[0].items()\n",
        "            }\n",
        "        )\n",
        "\n",
        "        encoded_inputs = self.tokenizer(\n",
        "            batch[self.config[\"input_column\"]], \n",
        "            max_length = 120, \n",
        "            padding=True, \n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        encoded_targets = self.tokenizer(\n",
        "            batch[self.config[\"output_column\"]], max_length = 120, padding=True, truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        encoded_inputs[\"labels\"] = encoded_targets[\"input_ids\"]\n",
        "\n",
        "        return encoded_inputs\n",
        "\n",
        "collator = SimpleCollator(tokenizer, {\"input_column\": \"prompt_input\", \"output_column\": \"output\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Prepare Dataloaders\n",
        "train_dl = DataLoader(\n",
        "    train, \n",
        "    batch_size=4,\n",
        "    pin_memory=True,\n",
        "    shuffle=False,\n",
        "    collate_fn=collator\n",
        ")\n",
        "\n",
        "val_dl = DataLoader(\n",
        "    val,\n",
        "    batch_size=16,\n",
        "    pin_memory=True,\n",
        "    shuffle=True,\n",
        "    collate_fn=collator\n",
        ")\n",
        "\n",
        "test_dl = DataLoader(\n",
        "    test, \n",
        "    batch_size=16,\n",
        "    pin_memory=True,\n",
        "    shuffle=False,\n",
        "    collate_fn=collator\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Zero-shot baseline\n",
        "Let's first do a zero-shot inference pass for baseline reading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "58da470452144da7ba54690d681bf64b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/32 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/envs/nlp/lib/python3.10/site-packages/transformers/generation/utils.py:1659: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['(a)',\n",
              " '(a)',\n",
              " '(b)',\n",
              " '(a)',\n",
              " '(a)',\n",
              " '(b) she jumped rope',\n",
              " '(a)',\n",
              " '(a)',\n",
              " '(a)',\n",
              " '(b)',\n",
              " '(a)',\n",
              " '(b)',\n",
              " '(a)',\n",
              " '(a)',\n",
              " '(a)']"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tqdm.notebook as tqdm\n",
        "\n",
        "all_preds = []\n",
        "for batch in tqdm.tqdm(test_dl, total = len(test_dl)):\n",
        "    \n",
        "    preds = model.generate(**batch, max_new_tokens=25)\n",
        "    outputs = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "    all_preds.extend(outputs)\n",
        "\n",
        "# Note the FlAN T5 model ignores our instruction format and procuces the letters for prediction\n",
        "all_preds[:15]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9ff3e599c3b44d20ba9d12ed1fe4f267",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/1500 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "test_df= data.filter(lambda x: x[\"split\"] == \"test\").to_pandas()\n",
        "test_df[\"baseline_preds\"] = all_preds\n",
        "\n",
        "# Let's manually score each example\n",
        "baseline_is_correct = []\n",
        "for i, row in test_df.iterrows():\n",
        "    \n",
        "    if \"a)\" in row[\"baseline_preds\"]:\n",
        "        pred = 1\n",
        "    else:\n",
        "        pred = 2\n",
        "\n",
        "    baseline_is_correct.append(pred == row[\"answer\"])\n",
        "\n",
        "test_df[\"baseline_is_correct\"] = baseline_is_correct\n",
        "\n",
        "print(f\"Baseline accuracy: {test_df['baseline_is_correct'].mean()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Finetuning\n",
        "Note that the baseline accuracy is very high. Finetuning may actually make things worse!\n",
        "\n",
        "For finetuning we ues the Pytorch lightning library which makes build finetuning loops very easy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "df9ef43a22ec454c8280e9cb707079ff",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import lightning as pl\n",
        "from torch.optim import AdamW \n",
        "\n",
        "class PeftCALMT5(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, model_alias: str, tokenizer_alias: str):\n",
        "\n",
        "        super().__init__()\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_alias)\n",
        "\n",
        "        self.peft_config = LoraConfig(\n",
        "            task_type=TaskType.SEQ_2_SEQ_LM, \n",
        "            inference_mode=False, \n",
        "            target_modules=[\"q\", \"k\", \"v\"],\n",
        "            r=8, \n",
        "            lora_alpha=32, \n",
        "            lora_dropout=0.5\n",
        "        )\n",
        "\n",
        "        model = AutoModelForSeq2SeqLM.from_pretrained(model_alias)\n",
        "        self.model = get_peft_model(model, self.peft_config)\n",
        "        \n",
        "\n",
        "    def training_step(self, batch, batch_idx): \n",
        "        outputs = self.model.forward(**batch, return_dict=True)\n",
        "        loss = outputs[\"loss\"]  \n",
        "        \n",
        "        self.log(\"train_loss\", loss, prog_bar=True, on_step=True, on_epoch=True)     \n",
        "        return loss\n",
        "    \n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        outputs = self.model(**batch)\n",
        "        loss = outputs[\"loss\"]  \n",
        "        \n",
        "        self.log(\"val_loss\", loss, prog_bar=True, on_step=False, on_epoch=True) \n",
        "        \n",
        "    def configure_optimizers(self):\n",
        "        optimizer = AdamW(self.model.parameters(), lr=5e-4)\n",
        "        return optimizer\n",
        "\n",
        "\n",
        "model = PeftCALMT5(model_alias=\"google/flan-t5-xl\", tokenizer_alias=\"google/flan-t5-xl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name  | Type                  | Params\n",
            "------------------------------------------------\n",
            "0 | model | PeftModelForSeq2SeqLM | 2.9 B \n",
            "------------------------------------------------\n",
            "7.1 M     Trainable params\n",
            "2.8 B     Non-trainable params\n",
            "2.9 B     Total params\n",
            "11,427.340Total estimated model params size (MB)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "23c347f374084311a00aaa8b4f1d0375",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3ac81a976e7e412bb46cf609598d1355",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bdddc4ae139c443f924472e7560eabba",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
          ]
        }
      ],
      "source": [
        "trainer = pl.Trainer(\n",
        "  max_epochs=1,\n",
        "  devices=1, \n",
        "  accelerator=\"gpu\",\n",
        "  accumulate_grad_batches=3 # Note we accumlate batches to effective form larger training batches \n",
        ")\n",
        "\n",
        "trainer.fit(model, train_dl, val_dl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "70a7d44cf4c047b1bdc34b9edb73a2ac",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/32 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['it was fragile', 'i retrieved a ticket stub', 'the termites ate through the wood in the house', 'the patrol agent checked their passports', 'it was a holiday', 'she jumped rope', 'more people entered the line', 'the baby drooled on her bib', 'the audience clapped along to the music', 'the girl brought the teacher an apple']\n"
          ]
        }
      ],
      "source": [
        "ft_preds = []\n",
        "for batch in tqdm.tqdm(test_dl, total = len(test_dl)):\n",
        "    preds = model.model.generate(**batch, max_new_tokens=25)\n",
        "    outputs = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "    ft_preds.extend(outputs)\n",
        "print(ft_preds[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LoRA FT accuracy: 0.952\n"
          ]
        }
      ],
      "source": [
        "test_df[\"lora_pred\"] = ft_preds\n",
        "test_df[\"lora_is_correct\"] = test_df.apply(lambda x: int(x['lora_pred'].lower().strip() == x[\"output\"].lower().strip()), axis=1)\n",
        "print(f\"LoRA FT accuracy: {test_df['lora_is_correct'].mean()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
