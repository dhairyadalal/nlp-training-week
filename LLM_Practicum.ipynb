{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrcN1HP62Dc3"
      },
      "source": [
        "# Notebook Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFpLonhS2G-r",
        "outputId": "6a8e5db3-433b-4300-b68c-fb5fd60c16f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed setup\n"
          ]
        }
      ],
      "source": [
        "!pip install onnxruntime >> NULL\n",
        "!pip install -U torch >> NULL\n",
        "!pip install -U sentence_transformers >> NULL\n",
        "!pip install -q -U einops tiktoken accelerate peft bitsandbytes transformers >> NULL\n",
        "\n",
        "print(\"Completed setup\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrL-jQaC2mQg"
      },
      "source": [
        "# LLM Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "hO5QTE7d2pDs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from transformers import BitsAndBytesConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLb1JCVA278R"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "# Speciy model alias for HF\n",
        "alias = \"NousResearch/Llama-2-7b-chat-hf\"\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(alias, trust_remote_code=True)\n",
        "\n",
        "# Quantization Config\n",
        "# quant_config = BitsAndBytesConfig(\n",
        "#    load_in_4bit=True,\n",
        "#    bnb_4bit_quant_type=\"nf4\",\n",
        "#    bnb_4bit_use_double_quant=True,\n",
        "#    #bnb_4bit_compute_dtype=torch.bfloat16\n",
        "# )\n",
        "\n",
        "# Load Model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    alias,\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=\"auto\",\n",
        "    #quantization_config=quant_config\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0FpLi3o3_Kt"
      },
      "outputs": [],
      "source": [
        "from string import Template\n",
        "\n",
        "prompt_template = Template(\n",
        "    \"\"\"\n",
        "    <s>[INST] <<SYS>>\n",
        "    You are a helpful chatbot.\n",
        "    $input\n",
        "    <</SYS>>\n",
        "    Answer the provided question. Be concise and clear in your response.\n",
        "    [/INST]\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "input = \"Mary has 10 apples. She give 3 to John and 1 to Bob. She throws away 1 more apple. How many apples does Mary have left?\"\n",
        "\n",
        "prompt = prompt_template.substitute({\"input\": input})\n",
        "encoded_prompt = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "output = model.generate(**encoded_prompt, max_new_tokens=150)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2d7WW3JY_tiS"
      },
      "outputs": [],
      "source": [
        "print(tokenizer.decode(output[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## COT style prompting"
      ],
      "metadata": {
        "id": "s4aS5j55ZOlB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cot_template = Template(\n",
        "  \"\"\"\n",
        "  <s>[INST] <<SYS>>\n",
        "  You are a helpful chatbot.\n",
        "  $input\n",
        "  <</SYS>>\n",
        "  Answer the provided question. Let's think step-by-step. Provide an your reasoning steps and then answer the question.\n",
        "  Be concise and clear in your response.\n",
        "  [/INST]\n",
        "  \"\"\"\n",
        ")\n",
        "\n",
        "input = \"Mary has 10 apples. She give 3 to John and 1 to Bob. She throws away 1 more apple. How many apples does Mary have left?\"\n",
        "\n",
        "prompt = cot_template.substitute({\"input\": input})\n",
        "encoded_prompt = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "output = model.generate(**encoded_prompt, max_new_tokens=150)\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "OIeXXh5yZRhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG Pipeline\n",
        "\n",
        "## Semantic Similarity Basics"
      ],
      "metadata": {
        "id": "TXrl6K3jPZoX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "s1 = \"Cats are super cool.\"\n",
        "s2 = \"Cats are awesome.\"\n",
        "s3 = \"I like felines.\"\n",
        "s4 = \"Centipedes are terrifying.\"\n",
        "\n",
        "s1_embed = embedder.encode(s1).reshape(1,-1)\n",
        "s2_embed = embedder.encode(s2).reshape(1, -1)\n",
        "s3_embed = embedder.encode(s3).reshape(1, -1)\n",
        "s4_embed = embedder.encode(s4).reshape(1, -1)\n",
        "\n",
        "\n",
        "print(f\"Cosine similarity between s1 and s: {cosine_similarity(s1_embed, s2_embed)}\")\n",
        "print(f\"Cosine similarity between s1 and s3: {cosine_similarity(s1_embed, s3_embed)}\")\n",
        "print(f\"Cosine similarity between s1 and s4: {cosine_similarity(s1_embed, s4_embed)}\")\n",
        "\n",
        "# Semantic similarity for retrieval\n",
        "query = \"What is super cool?\"\n",
        "query_embed = embedder.encode(query).reshape(1, -1)\n",
        "\n",
        "print(f\"s1 relevance for query: {cosine_similarity(query_embed, s1_embed)}\")\n",
        "print(f\"s2 relevance for query: {cosine_similarity(query_embed, s2_embed)}\")\n",
        "print(f\"s3 relevance for query: {cosine_similarity(query_embed, s3_embed)}\")\n",
        "print(f\"s4 relevance for query: {cosine_similarity(query_embed, s4_embed)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dl4kFnuePg7d",
        "outputId": "584698c6-3f9b-40e1-d861-9e58a2b0b707"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine similarity between s1 and s: [[0.8596046]]\n",
            "Cosine similarity between s1 and s3: [[0.5692848]]\n",
            "Cosine similarity between s1 and s4: [[0.3089841]]\n",
            "s1 relevance for query: [[0.5085266]]\n",
            "s2 relevance for query: [[0.26878813]]\n",
            "s3 relevance for query: [[0.17692326]]\n",
            "s4 relevance for query: [[0.14233847]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple Dense Passage Retrieval"
      ],
      "metadata": {
        "id": "uf2yU-N4Q_5I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "from typing import List\n",
        "\n",
        "class SimpleVectorDB:\n",
        "\n",
        "  def __init__(self, documents: List, embedder_alias: str):\n",
        "    self.documents = documents\n",
        "\n",
        "    # Build Index\n",
        "    # a. define the embedder\n",
        "    self.embedder = SentenceTransformer(embedder_alias)\n",
        "\n",
        "    # b. Embed the documents\n",
        "    self.kb = self.embedder.encode(documents)\n",
        "\n",
        "\n",
        "  def fetch_knowledge(self, query: str, n_results: int = 1) -> List[str]:\n",
        "    \"\"\"\n",
        "    Given a user query, retrieve the most relevant document from KB. Retrieval\n",
        "    should be based on the document which is the most semantic relevant to the\n",
        "    query as measured by cosine similarity. Return the top n_results.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Embed the query\n",
        "    query_embed = self.embedder.encode(query).reshape(1,-1)\n",
        "\n",
        "    # 2. Compute similarity between query and docs embeddings\n",
        "    sim_scores = cosine_similarity(query_embed, self.kb)\n",
        "\n",
        "    # 3. Sort documents based on cosine similarity score, order list from\n",
        "    # most similar to least\n",
        "    sorted_doc_ids = np.argsort(sim_scores)[0][::-1]\n",
        "\n",
        "    # # 4. Retrieve top n documents based on sorted document id\n",
        "    final_docs = [ self.documents[i] for i in sorted_doc_ids ][:n_results]\n",
        "\n",
        "    return final_docs\n",
        "\n"
      ],
      "metadata": {
        "id": "yOE93CJjQ_RP"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate Retrieval\n",
        "\n",
        "documents = [\n",
        "    \"The giraffe has 5 spots and 100 stripes.\",\n",
        "    \"The giraffe has blue eyes.\",\n",
        "    \"Giraffes have 4 legs.\",\n",
        "    \"The cat has 6 spots and 200 stripes.\",\n",
        "    \"The cat as green eyes.\",\n",
        "    \"Cats have 4 legs and a tail.\",\n",
        "    \"Penguins have no spots and no stripes.\",\n",
        "    \"Penguins have 2 legs.\",\n",
        "    \"The penguin has emerald eyes\"\n",
        "]\n",
        "\n",
        "# Intialize kb\n",
        "kb = SimpleVectorDB(documents, \"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Sample queries\n",
        "q1 = \"Which animal has blue eyes?\"\n",
        "expected_document = \"The giraffe has blue eyes.\"\n",
        "\n",
        "print(q1)\n",
        "print(\"Top documents: \", kb.fetch_knowledge(q1))\n",
        "print(f\"Check: {kb.fetch_knowledge(q1)[0] == expected_document}\")\n",
        "\n",
        "\n",
        "q2 = \"Which animals have atleast 2 legs?\"\n",
        "expected_documents = ['Penguins have 2 legs.', 'Giraffes have 4 legs.', 'Cats have 4 legs and a tail.']\n",
        "print(q2)\n",
        "print(\"Top documents: \", kb.fetch_knowledge(q2, 3))\n",
        "print(f\"Check: { len(set(kb.fetch_knowledge(q2, 3)).intersection(expected_documents)) == 3 }\")\n"
      ],
      "metadata": {
        "id": "WdRBaJjZryhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval + Generation"
      ],
      "metadata": {
        "id": "F5FDW7ET1Ip0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\", device_map=\"auto\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HrmeSgR24gi",
        "outputId": "a2910eb6-38bd-4bf8-8504-b71fec10f8b0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from string import Template\n",
        "\n",
        "# 1. Retrieve the top documents\n",
        "q1 = \"Which animal has blue eyes?\"\n",
        "docs =  kb.fetch_knowledge(q1,3)\n",
        "\n",
        "# 2. Construct prompt with in-context information\n",
        "prompt_template = Template(\n",
        "\"\"\"\n",
        "Answer the provided question below using the provided context.\n",
        "Context: $context\n",
        "Question: $question\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "# 3. Build context string by appending each documents with \\n seperator\n",
        "context = \"\\n\".join(docs)\n",
        "prompt = prompt_template.substitute({\"context\": context, \"question\": q1})\n",
        "\n",
        "print(\"prompt\")\n",
        "print(prompt)\n",
        "\n",
        "print('----------')\n",
        "# 4. Prompt model\n",
        "encoded_input = tokenizer(prompt, return_tensors=\"pt\")\n",
        "outputs = model.generate(**encoded_input, max_new_tokens=20)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "LWAGG6D61IBs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}